{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Translation-Transformers.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ofnIQRNaCJYe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import TextVectorization, Dense, MaxPooling1D, Conv1D, LSTM, MultiHeadAttention, Flatten, Layer, LayerNormalization, Dropout, Embedding\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "jJUgQnE1GX-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT_LOUyxGZqZ",
        "outputId": "f015c213-e37d-41f3-ce63-86de0a58c883"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_path = \"/content/drive/My Drive/Machine Learning Datasets/Hindi-English\""
      ],
      "metadata": {
        "id": "k2W3KVeTGyig"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Hindi_embedding_matrix = np.load(datasets_path + '/Hindi_embedding_matrix.npy')\n",
        "English_embedding_matrix = np.load(datasets_path + '/English_embedding_matrix.npy')\n",
        "\n",
        "Hindi_vectorized_text = np.load(datasets_path + '/Hindi_vectorized.npy')\n",
        "English_vectorized_text = np.load(datasets_path + '/English_vectorized.npy')"
      ],
      "metadata": {
        "id": "RP7_raV5HIh6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"\"\"\n",
        "    Hindi Embeddings Shape =  {Hindi_embedding_matrix.shape}\n",
        "    English Embeddings shape = {English_embedding_matrix.shape}\n",
        "\n",
        "    Hindi_vectors_shape = {Hindi_vectorized_text.shape}\n",
        "    English_vectors_shape = {English_vectorized_text.shape}\n",
        "    \"\"\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnJEVpovHph5",
        "outputId": "aa9be7d3-5aed-40ee-aa65-cc461e371252"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    Hindi Embeddings Shape =  (84469, 300)\n",
            "    English Embeddings shape = (76776, 300)\n",
            "\n",
            "    Hindi_vectors_shape = (127575, 417)\n",
            "    English_vectors_shape = (127575, 398)\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Note-\n",
        "    -> Embeddings Dimensions for both Hindi and English texts = 300\n",
        "    -> English texts have a vocabulary of 76776, while Hindi texts have a vocabulary of 84469\n",
        "    -> The maximum number of words in English texts is 398, while for Hindi, it's 417\n",
        "    -> There are 127_575 samples in parallel for both the features(English) and targets(Hindi)"
      ],
      "metadata": {
        "id": "_YxrG8AkIhyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 1024\n",
        "MAX_SEQ_LEN = 450\n",
        "EMBEDDING_DIMS = 300\n",
        "HINDI_VOCAB_SIZE = Hindi_embedding_matrix.shape[0]\n",
        "ENGLISH_VOCAB_SIZE = English_embedding_matrix.shape[0]"
      ],
      "metadata": {
        "id": "I9IBFsVeQtvy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((English_vectorized_text, Hindi_vectorized_text))\n",
        "dataset = dataset.shuffle(200_000)\n",
        "train = dataset.take(100_000)\n",
        "test = dataset.skip(100_000)\n",
        "train = train.batch(BATCH_SIZE)\n",
        "test = test.batch(1024)\n"
      ],
      "metadata": {
        "id": "Q0oBGG5bLKNl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> Masking </center>\n",
        "<p aligh = \"justify\" >Masking is a very simple but important factor in improving the accuracy of a model. It helps to tell a model, which parts of the input data to consider, at a particular Timestep. </p>\n",
        "\n",
        "<br>\n",
        "\n",
        "### <center> Types Of Masks </center>\n",
        "\n",
        "####  Padding Mask\n",
        "<p align = \"justify\">\n",
        "We have already padded our sequences towards the end, to keep the sequence size consistent over the training examples. But, if this data is fed directly to the model, it might get something from the padded portions, which don't even exist in the sequence. So, we send along with our inputs, a padding mask function, that masks the padded region.\n",
        "</p>\n",
        "\n",
        "####  Look Ahead Mask\n",
        "<p align = \"justify\">\n",
        "The Transformer Network has an Encoder-Decoder Architecture. The decoder architecture tries to predict the text for next timestep, only considering the past timesteps. Look Ahead Mask allows us to mask the upcoming sections of the text.\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "VbU0mrIBhrYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    ## Adding required dimensions to the padding\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]        # (Batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Working Example\n",
        "x = tf.constant([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
        "create_padding_mask(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwaBCSA1iPpS",
        "outputId": "f835ec13-5f2c-45ea-907a-356c65ce11ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 1, 5), dtype=float32, numpy=\n",
              "array([[[[0., 0., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "       [[[0., 0., 0., 1., 1.]]],\n",
              "\n",
              "\n",
              "       [[[1., 1., 1., 0., 0.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_lookahead_mask(size):\n",
        "    n = int(size * (size+1) / 2)\n",
        "    mask = tfp.math.fill_triangular(tf.ones((n,), dtype=tf.int32), upper=False)\n",
        "    return tf.cast(mask, tf.float32)\n",
        "\n",
        "# Working Example\n",
        "x = tf.random.uniform((1, 3))\n",
        "temp = create_lookahead_mask(x.shape[1])\n",
        "print(x)\n",
        "print(temp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeAyZ5v6kp6a",
        "outputId": "40b1dbf2-3df1-4c0e-bd38-81e666720339"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([[0.8253505  0.84161127 0.8758038 ]], shape=(1, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 0. 0.]\n",
            " [1. 1. 0.]\n",
            " [1. 1. 1.]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center> Positional Encoding </center>\n",
        "<p align = \"justify\">\n",
        "Attention layers see their input as a set of vectors, with no sequential order. This model also doesn't contain any recurrent or convolutional layers. Because of this a \"positional encoding\" is added to give the model some information about the relative position of the tokens in the sentence.<br>\n",
        "The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of tokens in a sentence. So after adding the positional encoding, tokens will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space.\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$\n",
        "<center>\n",
        "<img src = \"https://camo.githubusercontent.com/c279dcae2225189217ab2827711b56b10919d179aead5584e87426de6ee27a67/68747470733a2f2f6a696e676c6573636f64652e6769746875622e696f2f6173736574732f696d672f706f7374732f696c6c75737472617465642d67756964652d7472616e73666f726d65722d31302e6a7067\" width = \"800\"> </center>"
      ],
      "metadata": {
        "id": "KJbGiOgNvAkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_positional_encoding(num_positions:int, dimensions:int):\n",
        "    \"\"\"\n",
        "    num_positions: Length Of Sequences in the dataset after padding\n",
        "    dimensions: Number of dimensions used to represent each word in embedding matrix\n",
        "    \"\"\"\n",
        "    # Create a column vector for positions\n",
        "    pos_vec = np.arange(num_positions)[:, np.newaxis]\n",
        "    \n",
        "    # Create a row vector for dimensions\n",
        "    dims_vec = np.arange(dimensions)[np.newaxis, :]\n",
        "    \n",
        "    i = dims_vec // 2\n",
        "    angles = pos_vec * 1.0 / (np.power(10_000, 2 * i / np.float32(dimensions)))\n",
        "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
        "    angles[:, 0::1] = np.cos(angles[:, 0::1])\n",
        "    pos_encoding = angles[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, tf.float32)\n",
        "\n",
        "get_positional_encoding(10, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uylmRIAHC6oy",
        "outputId": "9c1b6be0-9f53-47ce-c6e6-349b6075d98a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 10, 4), dtype=float32, numpy=\n",
              "array([[[ 1.        ,  1.        ,  1.        ,  1.        ],\n",
              "        [ 0.66636676,  0.5403023 ,  0.99995   ,  0.99995   ],\n",
              "        [ 0.6143003 , -0.41614684,  0.9998    ,  0.9998    ],\n",
              "        [ 0.9900591 , -0.9899925 ,  0.99955016,  0.99955004],\n",
              "        [ 0.7270351 , -0.6536436 ,  0.9992005 ,  0.9992001 ],\n",
              "        [ 0.5744009 ,  0.2836622 ,  0.9987513 ,  0.99875027],\n",
              "        [ 0.9612168 ,  0.96017027,  0.9982027 ,  0.99820054],\n",
              "        [ 0.7918362 ,  0.75390226,  0.997555  ,  0.997551  ],\n",
              "        [ 0.5492263 , -0.14550003,  0.9968085 ,  0.99680173],\n",
              "        [ 0.9162743 , -0.91113025,  0.99596363,  0.9959527 ]]],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <center> Transformer </center>\n",
        "<center>\n",
        "<img src = \"https://miro.medium.com/max/642/1*oW9WHT_EAbcgSodkTMTTLA.png\" height = 400></img>"
      ],
      "metadata": {
        "id": "2lZRC5lgzX-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Encoder\n",
        "<p align = \"justify\">\n",
        "The Transformer Encoder layer pairs self-attention and convolutional neural network style of processing to improve the speed of training and passes K and V matrices to the Decoder. The inputs are passed through multiple layers of similar structure, to get the outputs to be fed to Decoder. The dimensions of the inputs are secured as they are, because blocks are repeated.\n",
        "</p>\n",
        "###### Steps-\n",
        "    -> Input Sequence is\n",
        "\n",
        "<img src = \"https://jalammar.github.io/images/t/transformer_resideual_layer_norm.png\" height = \"300\"> </img>"
      ],
      "metadata": {
        "id": "Yuy6qMpHSZix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder_Layer(Layer):\n",
        "    def __init__(self, embedding_dims, num_heads, fully_connected_dim,\n",
        "                 dropout_rate = 0.1, layernorm_eps = 1e-6):\n",
        "        super(Encoder_Layer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(num_heads,\n",
        "                                      key_dim = embedding_dims,\n",
        "                                      dropout = dropout_rate)\n",
        "        self.dense1 = Dense(fully_connected_dim, activation = 'relu')\n",
        "        self.dense2 = Dense(embedding_dims)\n",
        "        \n",
        "        self.layer_norm1 = LayerNormalization(epsilon = layernorm_eps)\n",
        "        self.layer_norm2 = LayerNormalization(epsilon = layernorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, mask):\n",
        "        attnout = self.mha(x, x, x, mask)\n",
        "        attnout = self.layer_norm1(x + attnout)\n",
        "\n",
        "        ffn_out = self.dense1(attnout)\n",
        "        ffn_out = self.dense2(attnout)\n",
        "        ffn_out = self.dropout_ffn(ffn_out)\n",
        "        encoder_layer_out = self.layer_norm2(ffn_out + attnout)\n",
        "        return encoder_layer_out"
      ],
      "metadata": {
        "id": "BEVbnRnQl3qR"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(Layer):\n",
        "    def __init__(self, num_layers, num_heads,\n",
        "                 embedding_dims, fully_connencted_dim,\n",
        "                 input_vocab_size, sequence_len,\n",
        "                 dropout_rate = 0.1, layernorm_eps = 1e-6):\n",
        "        super(Encoder, self).__init__()\n",
        "        \n",
        "        self.embedding_dims = embedding_dims\n",
        "        self.num_layers = num_layers\n",
        "        self.sequence_len = sequence_len\n",
        "        self.embedding = Embedding(input_vocab_size, embedding_dims,\n",
        "                            embeddings_initializer = tf.keras.initializers.Constant(English_embedding_matrix))\n",
        "        \n",
        "        self.positional_encoding = get_positional_encoding(self.sequence_len, embedding_dims)\n",
        "\n",
        "        self.encoding_layers = [Encoder_Layer(\n",
        "                                    embedding_dims = embedding_dims,\n",
        "                                    num_heads = num_heads,                    \n",
        "                                    fully_connected_dim = fully_connencted_dim,                                \n",
        "                                    dropout_rate = dropout_rate, layernorm_eps = layernorm_eps\n",
        "                                ) for i in range(self.num_layers)]\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def call(self, x, padding_mask):\n",
        "\n",
        "        x = self.embedding(x)\n",
        "\n",
        "        ## Scaling the embeddings\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dims, tf.float32))\n",
        "\n",
        "        ## Adding the positional encodings to embeddings\n",
        "        x += self.positional_encoding[:, :self.sequence_len, :]\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # Passing x through series of encoder_layers:\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.encoding_layers[i](x, mask = padding_mask)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "PcU1o7zN3F9k"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decoder\n",
        "<p align = \"justify\">\n",
        "Similar to the Encoder, Decoder has multiple decoder_layers.\n",
        "</p>\n",
        "\n",
        "<img src = \"https://www.mihaileric.com/static/output_token-06e143fae69ea58572608d65a546255c-06aef.png\" height = 400></img>"
      ],
      "metadata": {
        "id": "5l8b2ca-ViE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder_Layer(Layer):\n",
        "    def __init__(self, num_heads, embedding_dims, fully_connected_dim, dropout_rate = 0.1, layernorm_eps = 1e-6):\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(num_heads, embedding_dims, dropout = dropout_rate)\n",
        "        self.layernorm1 = LayerNormalization(epsilon = layernorm_eps)\n",
        "\n",
        "        self.mha2 = MultiHeadAttention(num_heads, embedding_dims, dropout = dropout_rate)\n",
        "        self.layernorm2 = LayerNormalization(epsilon = layernorm_eps)\n",
        "\n",
        "        self.dense1 = Dense(fully_connected_dim, activation = 'relu')\n",
        "        self.dense2 = Dense(embedding_dims)\n",
        "        self.dropout_ = Dropout(dropout_rate)\n",
        "        self.layernorm3 = LayerNormalization(epsilon = layernorm_eps)\n",
        "\n",
        "    def call(self, x, encoder_out, padding_mask, lookahead_mask):\n",
        "        attn1_out = self.mha1(x, x, x, mask = lookahead_mask)\n",
        "        attn1_out = self.layernorm1(x + attn1_out)\n",
        "\n",
        "        attn2_out = self.mha2(attn1_out, encoder_out, encoder_out, mask = padding_mask)\n",
        "        attn2_out = self.layernorm2(attn1_out + attn2_out)\n",
        "\n",
        "        dense_out = self.dense1(attn2_out)\n",
        "        dense_out = self.dense2(dense_out)\n",
        "        dense_out = self.dropout_(dense_out)\n",
        "\n",
        "        decoder_out = self.layernorm3(dense_out + attn2_out)\n",
        "        return decoder_out\n"
      ],
      "metadata": {
        "id": "yDBZi-NNMOYc"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(Layer):\n",
        "    def __init__(self, num_layers, num_heads,\n",
        "                 embedding_dims, fully_connected_dim,\n",
        "                 output_vocab_size, sequence_length,\n",
        "                 dropout_rate = 0.1,\n",
        "                 layernorm_eps = 1e-6\n",
        "                 ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.seq_len = sequence_length\n",
        "        self.embedding_dims = embedding_dims\n",
        "\n",
        "        self.embedding = Embedding(output_vocab_size, embedding_dims,\n",
        "                            embeddings_initializer = tf.keras.initializers.Constant(English_embedding_matrix))\n",
        "        self.positional_encoding = get_positional_encoding(sequence_length, embedding_dims)\n",
        "\n",
        "        self.decoder_layers = [Decoder_Layer(num_heads, embedding_dims,\n",
        "                                              fully_connected_dim, dropout_rate,\n",
        "                                              layernorm_eps) for i in range(num_layers)]\n",
        "\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x, encoder_out, padding_mask, lookahead_mask):\n",
        "        x = self.embeddings(x)\n",
        "        x *= tf.math.sqrt(tf.cast(self.embedding_dims, tf.float32))\n",
        "        x += self.positional_encoding[:, :self.seq_len, :]\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.decoder_layers[i]( x, encoder_out, padding_mask, lookahead_mask)\n",
        "\n",
        "        return x\n",
        "        \n"
      ],
      "metadata": {
        "id": "s8JWwgIqjrdX"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "M7X8vcyM3NH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZK9OpuHLFhn6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}